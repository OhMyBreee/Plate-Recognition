{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation of some requirements\n",
    "# %pip install albumentations\n",
    "# %pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d9a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle dataset download\n",
    "# !kaggle datasets download juanthomaswijaya/indonesian-license-plate-dataset --path data --unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7438ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch # If you use PyTorch later for training\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Choose an integer seed (e.g., 42, 0, or any number)\n",
    "SEED = 42\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    # If you use PyTorch, uncomment these:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_all_seeds(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8793253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: test\n",
      "Processing directory: train\n",
      "Processing directory: val\n",
      "âœ… Data augmentation complete!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "from glob import glob\n",
    "\n",
    "# Paths\n",
    "INPUT_DATA_ROOT = \"../data/raw/Indonesian License Plate Dataset/\"\n",
    "\n",
    "input_images = os.path.join(INPUT_DATA_ROOT, \"images\")\n",
    "input_labels = os.path.join(INPUT_DATA_ROOT, \"labels\")\n",
    "\n",
    "# OUTPUT Paths (A new folder to store the augmented data, usually outside of 'raw')\n",
    "OUTPUT_DATA_ROOT = \"../data/processed/\"\n",
    "\n",
    "output_images = os.path.join(OUTPUT_DATA_ROOT, \"images\")\n",
    "output_labels = os.path.join(OUTPUT_DATA_ROOT, \"labels\")\n",
    "\n",
    "# --- END: Updated Paths ---\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(output_images, exist_ok=True)\n",
    "os.makedirs(output_labels, exist_ok=True)\n",
    "\n",
    "# Define augmentation pipeline\n",
    "transform = A.Compose([\n",
    "    A.Rotate(limit=5, p=0.5),\n",
    "    A.Perspective(scale=(0.02, 0.05)),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.MotionBlur(blur_limit=5, p=0.3),\n",
    "    A.RandomScale(scale_limit=0.2, p=0.5),\n",
    "],\n",
    "bbox_params=A.BboxParams(\n",
    "    format='yolo', \n",
    "    label_fields=['class_labels'],\n",
    "    # Add clip=True to ensure all coordinates are strictly within [0.0, 1.0]\n",
    "    clip=True,\n",
    "    min_area=1.0\n",
    "))\n",
    "\n",
    "# Loop through all images\n",
    "for subdir in os.listdir(input_images):\n",
    "    # Construct the full paths for the current subdirectory\n",
    "    current_input_image_dir = os.path.join(input_images, subdir)\n",
    "    current_input_label_dir = os.path.join(input_labels, subdir)\n",
    "\n",
    "    # ğŸ›‘ FIX 2: Create the corresponding output subdirectories\n",
    "    current_output_image_dir = os.path.join(output_images, subdir)\n",
    "    current_output_label_dir = os.path.join(output_labels, subdir)\n",
    "\n",
    "    os.makedirs(current_output_image_dir, exist_ok=True)\n",
    "    os.makedirs(current_output_label_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Processing directory: {subdir}\")\n",
    "\n",
    "    # Loop through all images in the subdirectory\n",
    "    # Using os.path.join is safer than f-string concatenation for paths\n",
    "    for img_path in glob(os.path.join(current_input_image_dir, \"*.jpg\")):\n",
    "        filename = os.path.basename(img_path)\n",
    "        \n",
    "        # ğŸ›‘ FIX 1: Correctly construct the label path using the subdirectory path\n",
    "        label_path = os.path.join(current_input_label_dir, filename.replace(\".jpg\", \".txt\"))\n",
    "\n",
    "        # Safety Check: Ensure the label file exists before proceeding\n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"Warning: Label file not found for {filename} in {subdir}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # Read image and labels (continue with your original logic)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            print(f\"Error: Could not read image {img_path}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        h, w, _ = image.shape # This is for context, not strictly needed by the augmentation code\n",
    "\n",
    "        # ... (rest of the label reading, augmentation, and saving logic) ...\n",
    "\n",
    "        with open(label_path, \"r\") as f:\n",
    "            labels = f.readlines()\n",
    "\n",
    "        bboxes = []\n",
    "        class_labels = []\n",
    "        for label in labels:\n",
    "            try:\n",
    "                cls, x, y, bw, bh = map(float, label.strip().split())\n",
    "                bboxes.append([x, y, bw, bh])\n",
    "                class_labels.append(int(cls))\n",
    "            except Exception as e:\n",
    "                 # Added error handling for malformed lines\n",
    "                print(f\"Error processing label line in {label_path}: {e}. Skipping image.\")\n",
    "                continue\n",
    "\n",
    "        # Apply transformation\n",
    "        transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "\n",
    "        aug_img = transformed['image']\n",
    "        aug_bboxes = transformed['bboxes']\n",
    "        aug_labels = transformed['class_labels']\n",
    "\n",
    "        # ğŸ›‘ FIX 2 (Final Save): Save augmented files into the correct OUTPUT subdirectory\n",
    "        \n",
    "        # Adding 'aug_' prefix is good practice to distinguish augmented files\n",
    "        aug_filename = f\"aug_{filename}\" \n",
    "\n",
    "        # Save augmented image\n",
    "        save_path_img = os.path.join(current_output_image_dir, aug_filename)\n",
    "        cv2.imwrite(save_path_img, aug_img)\n",
    "\n",
    "        # Save updated YOLO labels\n",
    "        save_path_lbl = os.path.join(current_output_label_dir, aug_filename.replace(\".jpg\", \".txt\"))\n",
    "        with open(save_path_lbl, \"w\") as f:\n",
    "            for cls, bbox in zip(aug_labels, aug_bboxes):\n",
    "                f.write(f\"{cls} {' '.join(map(str, bbox))}\\n\")\n",
    "\n",
    "print(\"âœ… Data augmentation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "608491d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO version: 1\n",
      "CUDA available: True\n",
      "Device being used: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "print(\"YOLO version:\", YOLO._version)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device being used:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8d23445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model device (PyTorch check): cuda:0\n",
      "Starting YOLOv8 training...\n",
      "New https://pypi.org/project/ultralytics/8.3.233 available ğŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.223 ğŸš€ Python-3.10.19 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=6, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=../data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=40, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=lp_detection_v22, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/notebooks/runs/detect/lp_detection_v22, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 1.6Â±0.2 ms, read: 199.0Â±24.1 MB/s, size: 2023.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/data/processed/labels/train.cache... 800 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 800/800 964.2Kit/s 0.0s\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.7Â±0.2 ms, read: 103.1Â±11.2 MB/s, size: 1821.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/data/processed/labels/val.cache... 100 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 100/100 49.1Kit/s 0.0s\n",
      "Plotting labels to /mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/notebooks/runs/detect/lp_detection_v22/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.000515625), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/notebooks/runs/detect/lp_detection_v22\u001b[0m\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/40       1.3G      1.556       2.83      1.149          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 6.1it/s 22.0s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 5.9it/s 1.5s0.2s\n",
      "                   all        100        179      0.798      0.728      0.823      0.498\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/40       1.3G      1.486       1.75      1.112          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.5it/s 15.7s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 6.5it/s 1.4s0.2s\n",
      "                   all        100        179      0.855       0.81      0.898      0.536\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/40      1.34G      1.503      1.549      1.128          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.8it/s 15.2s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 6.7it/s 1.3s0.2s\n",
      "                   all        100        179      0.869      0.844       0.91      0.521\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/40      1.35G      1.478      1.303      1.111          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.3it/s 16.1s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 8.2it/s 1.1s0.2s\n",
      "                   all        100        179      0.881      0.883      0.919       0.57\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/40      1.35G      1.428      1.183       1.09          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.3it/s 16.1s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 6.2it/s 1.4s0.2s\n",
      "                   all        100        179      0.928      0.844      0.918      0.536\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/40      1.35G      1.405      1.139      1.106          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.7it/s 15.4s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.3it/s 1.0s0.1s\n",
      "                   all        100        179      0.877       0.86       0.92      0.561\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/40      1.35G       1.41      1.083      1.103          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.3it/s 16.2s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 8.0it/s 1.1s0.1s\n",
      "                   all        100        179      0.947      0.855      0.946      0.586\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/40      1.35G      1.374     0.9633      1.077          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.5it/s 15.7s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 8.0it/s 1.1s0.2s\n",
      "                   all        100        179      0.936        0.9      0.957      0.548\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/40      1.35G      1.319     0.9287       1.06          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.0it/s 16.8s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 7.5it/s 1.2s0.2s\n",
      "                   all        100        179      0.946      0.916      0.974      0.627\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/40      1.35G      1.329     0.9163      1.068          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.5it/s 15.7s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 7.7it/s 1.2s0.1s\n",
      "                   all        100        179       0.93      0.939      0.963      0.619\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/40      1.35G      1.293     0.8847      1.045          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.3it/s 16.1s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.1it/s 1.0s0.1s\n",
      "                   all        100        179      0.928      0.942      0.957      0.639\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/40      1.35G      1.282      0.851      1.039          8        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.2it/s 16.3s0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.1it/s 1.0s0.1s\n",
      "                   all        100        179      0.914      0.955      0.972      0.651\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/40      1.35G      1.275     0.8606      1.038          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.6it/s 15.6s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 8.8it/s 1.0s0.1s\n",
      "                   all        100        179      0.926      0.914      0.956      0.645\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/40      1.35G      1.288     0.8401      1.044         15        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.4it/s 15.9s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 8.6it/s 1.0s0.1s\n",
      "                   all        100        179      0.935      0.939      0.968      0.652\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/40      1.35G      1.248     0.8234      1.022          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.4it/s 15.9s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.4it/s 1.0s0.1s\n",
      "                   all        100        179       0.94      0.933      0.976      0.654\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/40      1.35G      1.235      0.777      1.019          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.6it/s 15.6s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.5it/s 1.0s0.1s\n",
      "                   all        100        179      0.928      0.961      0.975      0.646\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/40      1.35G      1.244     0.7963      1.027          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.5it/s 15.7s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 8.0it/s 1.1s0.1s\n",
      "                   all        100        179      0.942      0.939      0.974       0.58\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/40      1.35G      1.226     0.7693      1.018          2        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.1it/s 16.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.0it/s 1.0s0.1s\n",
      "                   all        100        179      0.925      0.939      0.981       0.68\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/40      1.35G      1.218     0.7629      1.027          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.4it/s 16.0s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.4it/s 1.0s0.1s\n",
      "                   all        100        179      0.938       0.95       0.98      0.641\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/40      1.35G      1.224      0.746      1.006          9        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.8it/s 15.3s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 8.5it/s 1.1s0.1s\n",
      "                   all        100        179       0.92      0.964      0.984      0.658\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      21/40      1.35G      1.202     0.7265      1.006          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.4it/s 16.0s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.5it/s 0.9s0.1s\n",
      "                   all        100        179      0.944      0.944      0.976      0.676\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/40      1.35G      1.191     0.7316     0.9973         11        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.7it/s 15.3s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.3it/s 1.0s0.1s\n",
      "                   all        100        179      0.948       0.95      0.982      0.624\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/40      1.35G       1.17     0.7031     0.9898          7        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.0it/s 16.6s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.4it/s 1.0s0.1s\n",
      "                   all        100        179      0.944      0.945      0.977      0.696\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/40      1.35G      1.177     0.7046      1.007          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.4it/s 16.0s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 8.6it/s 1.0s0.1s\n",
      "                   all        100        179      0.955      0.955      0.984      0.644\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/40      1.35G      1.145     0.6678     0.9848          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.8it/s 15.2s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.3it/s 1.0s0.1s\n",
      "                   all        100        179      0.958      0.972      0.985       0.67\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      26/40      1.35G      1.147     0.6732     0.9885          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.2it/s 16.3s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.9it/s 0.9s0.1s\n",
      "                   all        100        179      0.944      0.966      0.983      0.693\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      27/40      1.35G      1.125     0.6645     0.9708          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.4it/s 15.9s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 8.5it/s 1.1s0.1s\n",
      "                   all        100        179       0.96      0.978      0.988      0.666\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      28/40      1.35G      1.132     0.6537      0.996          6        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.4it/s 16.0s0.3s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.3it/s 1.0s0.1s\n",
      "                   all        100        179      0.961      0.956      0.984      0.694\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      29/40      1.35G      1.151     0.6514      0.985          1        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.5it/s 15.7s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 8.4it/s 1.1s0.1s\n",
      "                   all        100        179      0.961      0.962      0.981      0.698\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      30/40      1.35G      1.085     0.6199     0.9591          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 7.9it/s 17.1s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 8.5it/s 1.1s0.1s\n",
      "                   all        100        179      0.972      0.954      0.987      0.678\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      31/40      1.35G      1.064     0.6037     0.9834          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 7.7it/s 17.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.2it/s 1.0s0.1s\n",
      "                   all        100        179      0.942      0.966      0.985      0.685\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      32/40      1.35G       1.05     0.5921     0.9669          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.6it/s 15.6s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.3it/s 1.0s0.1s\n",
      "                   all        100        179      0.961      0.976      0.989      0.663\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      33/40      1.35G      1.032     0.5824     0.9759          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.4it/s 16.0s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.0it/s 1.0s0.1s\n",
      "                   all        100        179      0.956      0.979      0.991      0.709\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      34/40      1.35G      1.027     0.5672      0.967          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.2it/s 16.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 8.4it/s 1.1s0.1s\n",
      "                   all        100        179      0.972      0.963      0.991      0.666\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      35/40      1.35G      1.011      0.555     0.9618          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.7it/s 15.5s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.0it/s 1.0s0.1s\n",
      "                   all        100        179      0.984      0.961      0.991      0.662\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      36/40      1.35G      1.013     0.5631     0.9685          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.1it/s 16.5s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 8.9it/s 1.0s0.1s\n",
      "                   all        100        179      0.976      0.972      0.992      0.663\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      37/40      1.35G     0.9854     0.5474     0.9509          4        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.7it/s 15.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.5it/s 0.9s0.1s\n",
      "                   all        100        179       0.96      0.978       0.99      0.703\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      38/40      1.35G     0.9718     0.5286     0.9494          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.5it/s 15.8s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 10.0it/s 0.9s.3s\n",
      "                   all        100        179      0.977      0.961       0.99      0.704\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      39/40      1.35G     0.9377     0.5024     0.9305          5        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.8it/s 15.3s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 10.6it/s 0.8s.1s\n",
      "                   all        100        179      0.955      0.972      0.991      0.707\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      40/40      1.35G     0.9626     0.5127     0.9436          3        640: 100% â”â”â”â”â”â”â”â”â”â”â”â” 134/134 8.6it/s 15.5s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 9.8it/s 0.9s0.1s\n",
      "                   all        100        179      0.961      0.971      0.991      0.694\n",
      "\n",
      "40 epochs completed in 0.197 hours.\n",
      "Optimizer stripped from /mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/notebooks/runs/detect/lp_detection_v22/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/notebooks/runs/detect/lp_detection_v22/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/notebooks/runs/detect/lp_detection_v22/weights/best.pt...\n",
      "Ultralytics 8.3.223 ğŸš€ Python-3.10.19 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 9/9 5.5it/s 1.6s0.2s\n",
      "                   all        100        179      0.956      0.979      0.991      0.709\n",
      "Speed: 0.6ms preprocess, 5.3ms inference, 0.0ms loss, 3.2ms postprocess per image\n",
      "Results saved to \u001b[1m/mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/notebooks/runs/detect/lp_detection_v22\u001b[0m\n",
      "Training complete. Results saved in runs/detect/lp_detection_v2\n"
     ]
    }
   ],
   "source": [
    "DATA_YAML_PATH = '../data.yaml'\n",
    "\n",
    "target_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Load a pre-trained YOLOv8-nano model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "model.model.to(target_device) \n",
    "\n",
    "# 3. Verify the change (using the reliable check)\n",
    "first_param_device = next(model.model.parameters()).device\n",
    "\n",
    "print(\"âœ… Model device (PyTorch check):\", first_param_device)\n",
    "\n",
    "print(\"Starting YOLOv8 training...\")\n",
    "results = model.train(\n",
    "    data=DATA_YAML_PATH,     # Path to the data configuration file\n",
    "    epochs=40,              # Number of epochs (adjust as needed)\n",
    "    imgsz=640,               # Input image size (standard for YOLO)\n",
    "    batch=6,                # Batch size (reduce if you run out of GPU memory)\n",
    "    name='lp_detection_v2',  # Name for the results folder in 'runs/detect\n",
    "    # Optional: use the 'freeze' argument if you want to freeze the backbone \n",
    "    amp=True,\n",
    "    device=0,  \n",
    "    lr0=1e-3\n",
    ")\n",
    "\n",
    "print(\"Training complete. Results saved in runs/detect/lp_detection_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "874142b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.223 ğŸš€ Python-3.10.19 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 2.2Â±0.3 ms, read: 141.0Â±11.5 MB/s, size: 1902.6 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/data/processed/labels/val.cache... 100 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 100/100 88.6Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 7/7 0.2it/s 29.1s0.2s\n",
      "                   all        100        179      0.956      0.979      0.991      0.715\n",
      "Speed: 32.1ms preprocess, 82.9ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1m/mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/notebooks/runs/detect/val3\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "evaluate = model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "077613e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['epoch', 'time', 'train/box_loss', 'train/cls_loss', 'train/dfl_loss', 'metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'val/box_loss', 'val/cls_loss', 'val/dfl_loss', 'lr/pg0', 'lr/pg1', 'lr/pg2']\n",
      "                              0          1          2          3          4\n",
      "epoch                  1.000000   2.000000   3.000000   4.000000   5.000000\n",
      "time                  20.588400  38.486100  56.903300  74.749800  94.643600\n",
      "train/box_loss         1.556410   1.486290   1.503490   1.477920   1.428210\n",
      "train/cls_loss         2.830230   1.750450   1.549060   1.302580   1.183260\n",
      "train/dfl_loss         1.148850   1.111730   1.127920   1.111330   1.089600\n",
      "metrics/precision(B)   0.797860   0.854530   0.868920   0.881150   0.928230\n",
      "metrics/recall(B)      0.727690   0.810060   0.843580   0.882680   0.843580\n",
      "metrics/mAP50(B)       0.822970   0.898340   0.910480   0.918690   0.918200\n",
      "metrics/mAP50-95(B)    0.498420   0.536240   0.520880   0.569900   0.535670\n",
      "val/box_loss           1.222400   1.350470   1.298310   1.231150   1.324140\n",
      "val/cls_loss           1.954200   1.217690   1.065170   1.052350   0.988720\n",
      "val/dfl_loss           1.033070   1.095580   1.058950   1.053360   1.063780\n",
      "lr/pg0                 0.000662   0.001295   0.001896   0.001852   0.001802\n",
      "lr/pg1                 0.000662   0.001295   0.001896   0.001852   0.001802\n",
      "lr/pg2                 0.000662   0.001295   0.001896   0.001852   0.001802\n",
      "Using metric column: metrics/mAP50(B)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"runs/detect/lp_detection_v2/results.csv\")\n",
    "\n",
    "# Inspect columns and head to see what's actually in the CSV\n",
    "print(\"columns:\", df.columns.tolist())\n",
    "print(df.head().T)\n",
    "\n",
    "# Pick the mAP column robustly\n",
    "map_cols = [c for c in df.columns if \"mAP50\" in c or \"map50\" in c.lower() or \"mAP\" in c]\n",
    "if not map_cols:\n",
    "    raise ValueError(\"No mAP column found in results.csv. Columns: \" + \", \".join(df.columns))\n",
    "col = map_cols[0]\n",
    "print(\"Using metric column:\", col)\n",
    "\n",
    "# Convert to numeric (coerce errors -> NaN) and warn if any NaNs\n",
    "y = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "if y.isnull().any():\n",
    "    print(\"Warning: some values in the metric column could not be converted to numeric and became NaN\")\n",
    "\n",
    "# Use epoch column if present else use dataframe index\n",
    "x = df[\"epoch\"] if \"epoch\" in df.columns else df.index\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(x, y, marker=\"o\")\n",
    "plt.xlabel(\"epoch\" if \"epoch\" in df.columns else \"index\")\n",
    "plt.ylabel(col)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "636a7695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "YOLO_MODEL_PATH = \"./runs/detect/lp_detection_v2/weights/best.pt\"\n",
    "INPUT_DIR = \"../data/processed/images/\"\n",
    "OUTPUT_DIR = \"../data/cropped/\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "model = YOLO(YOLO_MODEL_PATH)\n",
    "\n",
    "mapping = {}\n",
    "\n",
    "for class_name in os.listdir(INPUT_DIR):\n",
    "    input_class_dir = os.path.join(INPUT_DIR, class_name)\n",
    "    output_class_dir = os.path.join(OUTPUT_DIR, class_name)\n",
    "\n",
    "    # Create output directory for this class\n",
    "    os.makedirs(output_class_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_class_dir):\n",
    "        if not filename.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(input_class_dir, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        results = model.predict(img, verbose=False)\n",
    "        boxes = results[0].boxes\n",
    "\n",
    "        bboxes = boxes.xyxy.cpu().numpy()\n",
    "        confs = boxes.conf.cpu().numpy()\n",
    "\n",
    "        cropped_entries = []  # reset for each image\n",
    "\n",
    "        for i, (x1, y1, x2, y2) in enumerate(bboxes[:, :4]):\n",
    "            conf = confs[i]\n",
    "            if conf < 0.5:\n",
    "                continue\n",
    "\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "\n",
    "            crop_name = f\"{os.path.splitext(filename)[0]}_plate{i+1}.jpg\"\n",
    "            crop_path = os.path.join(output_class_dir, crop_name)\n",
    "\n",
    "            cv2.imwrite(crop_path, crop)\n",
    "\n",
    "            cropped_entries.append({\n",
    "                \"crop\": crop_name,\n",
    "                \"confidence\": float(conf),\n",
    "                \"bbox\": [x1, y1, x2, y2]\n",
    "            })\n",
    "\n",
    "        if cropped_entries:\n",
    "            mapping[filename] = cropped_entries\n",
    "\n",
    "# Save mapping once at the end\n",
    "mapping_file = os.path.join(OUTPUT_DIR, \"detections.json\")\n",
    "with open(mapping_file, \"w\") as f:\n",
    "    json.dump(mapping, f, indent=4)\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "544d7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def preprocess_for_ocr(img, target_h=32, target_w=128):\n",
    "    \"\"\"\n",
    "    Preprocess cropped license plate for OCR:\n",
    "    1. Grayscale\n",
    "    2. Denoise (bilateral filter)\n",
    "    3. Deskew (rotation estimation using moments)\n",
    "    4. Resize + Normalize\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Step 1: Convert to Grayscale ---\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # --- Step 2: Denoise ---\n",
    "    # Bilateral filter removes noise while keeping edges (better for characters)\n",
    "    denoised = cv2.bilateralFilter(gray, d=7, sigmaColor=30, sigmaSpace=30)\n",
    "\n",
    "    # --- Step 3: Deskew ---\n",
    "    # Calculate image moments to estimate skew angle\n",
    "    moments = cv2.moments(denoised)\n",
    "    if abs(moments[\"mu02\"]) < 1e-2:  \n",
    "        # Avoid division by zero for clean horizontal text\n",
    "        deskewed = denoised  \n",
    "    else:\n",
    "        skew = moments[\"mu11\"] / moments[\"mu02\"]\n",
    "        angle = math.degrees(math.atan(skew))\n",
    "\n",
    "        # Rotate opposite to deskew\n",
    "        h, w = denoised.shape\n",
    "        M = cv2.getRotationMatrix2D((w // 2, h // 2), angle, 1)\n",
    "        deskewed = cv2.warpAffine(denoised, M, (w, h), \n",
    "                                  flags=cv2.INTER_LINEAR, \n",
    "                                  borderMode=cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # --- Step 4: Resize for OCR ---\n",
    "    # CRNN expects height=32, width flexible (often 100â€“160)\n",
    "    resized = cv2.resize(deskewed, (target_w, target_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    # --- Step 5: Normalize (0â€“1 float32) ---\n",
    "    normalized = resized.astype(\"float32\") / 255.0\n",
    "\n",
    "    # Expand dims for OCR models expecting (1, H, W)\n",
    "    normalized = np.expand_dims(normalized, axis=0)  # shape: (1, 32, 128)\n",
    "\n",
    "    return np.squeeze(normalized)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22085bf",
   "metadata": {},
   "source": [
    "<h1> Preprocess OCR dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3692c9af",
   "metadata": {},
   "source": [
    "#item semua coy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d401922c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@3204.333] global loadsave.cpp:1063 imwrite_ Unsupported depth image for selected encoder is fallbacked to CV_8U.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m full_output_image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_category_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(full_cropped_image_path)\n\u001b[0;32m---> 13\u001b[0m processed_img \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_for_ocr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# if processed_img is None:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#     print(\"âŒ ERROR: preprocess_for_ocr() returned None for:\", full_cropped_image_path)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#     continue\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# # Save preprocessed output\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# # output_path = os.path.join(output_category_dir, filename)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimwrite(full_output_image_path, processed_img )\n",
      "Cell \u001b[0;32mIn[28], line 19\u001b[0m, in \u001b[0;36mpreprocess_for_ocr\u001b[0;34m(img, target_h, target_w)\u001b[0m\n\u001b[1;32m     15\u001b[0m gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(img, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# --- Step 2: Denoise ---\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Bilateral filter removes noise while keeping edges (better for characters)\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m denoised \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbilateralFilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmaColor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmaSpace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# --- Step 3: Deskew ---\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Calculate image moments to estimate skew angle\u001b[39;00m\n\u001b[1;32m     23\u001b[0m moments \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mmoments(denoised)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cropped_OCR_dir = \"../data/raw/Indonesian License Plate Recognition Dataset/images/\"\n",
    "output_OCR_dir = \"../data/trainOCR/images/\"\n",
    "os.makedirs(output_OCR_dir, exist_ok=True)\n",
    "\n",
    "for dataset_category in os.listdir(cropped_OCR_dir):\n",
    "    full_dataset_category = f\"{cropped_OCR_dir}{dataset_category}/\"\n",
    "    output_category_dir = f\"{output_OCR_dir}{dataset_category}/\"\n",
    "    os.makedirs(output_category_dir, exist_ok=True)\n",
    "    for filename in os.listdir(full_dataset_category):\n",
    "        full_cropped_image_path = f\"{full_dataset_category}{filename}\"\n",
    "        full_output_image_path = f\"{output_category_dir}{filename}\"\n",
    "        image = cv2.imread(full_cropped_image_path)\n",
    "        processed_img = preprocess_for_ocr(img)\n",
    "        # if processed_img is None:\n",
    "        #     print(\"âŒ ERROR: preprocess_for_ocr() returned None for:\", full_cropped_image_path)\n",
    "        #     continue\n",
    "\n",
    "        # if not isinstance(processed_img, (list, tuple)) and processed_img.size == 0:\n",
    "        #     print(\"âŒ ERROR: Empty processed image for:\", full_cropped_image_path)\n",
    "        #     continue\n",
    "\n",
    "        # print(\"Processed image shape:\", processed_img.shape)\n",
    "\n",
    "        # # Save preprocessed output\n",
    "        # # output_path = os.path.join(output_category_dir, filename)\n",
    "        cv2.imwrite(full_output_image_path, processed_img )\n",
    "\n",
    "print(\"\\nğŸ‰ All license plates have been preprocessed for OCR!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2355154",
   "metadata": {},
   "source": [
    "<h1> Import dataset for OCR recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e4b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¦ Loaded 0 images with labels.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 109\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    108\u001b[0m     data \u001b[38;5;241m=\u001b[39m load_ocr_dataset()\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)  \u001b[38;5;66;03m# Show first dataset entry\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "# ------------------------------\n",
    "# Paths\n",
    "# ------------------------------\n",
    "OCR_DATASET_DIR = \"../data/raw/Indonesian License Plate Recognition Dataset/\"\n",
    "IMAGES_DIR = os.path.join(OCR_DATASET_DIR, \"images/\")\n",
    "LABELS_DIR = os.path.join(OCR_DATASET_DIR, \"labels/\")\n",
    "CLASSES_FILE = os.path.join(OCR_DATASET_DIR, \"classes.names\")\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Load class names\n",
    "# ------------------------------\n",
    "def load_class_names(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        classes = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    return classes\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Convert YOLO (normalised) â†’ Pixel BBox\n",
    "# ------------------------------\n",
    "def yolo_to_xyxy(yolo_bbox, img_w, img_h):\n",
    "    \"\"\"\n",
    "    yolo_bbox = (cx, cy, w, h) normalized\n",
    "    return: x1, y1, x2, y2 in pixel coordinates\n",
    "    \"\"\"\n",
    "    cx, cy, w, h = yolo_bbox\n",
    "\n",
    "    # Convert to pixel values\n",
    "    cx *= img_w\n",
    "    cy *= img_h\n",
    "    w *= img_w\n",
    "    h *= img_h\n",
    "\n",
    "    x1 = int(cx - w / 2)\n",
    "    y1 = int(cy - h / 2)\n",
    "    x2 = int(cx + w / 2)\n",
    "    y2 = int(cy + h / 2)\n",
    "\n",
    "    return max(0, x1), max(0, y1), min(img_w, x2), min(img_h, y2)\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Load dataset\n",
    "# ------------------------------\n",
    "def load_ocr_dataset():\n",
    "    dataset = []\n",
    "\n",
    "    class_names = load_class_names(CLASSES_FILE)\n",
    "    for dataset_category in os.lisdir(IMAGES_DIR):\n",
    "        full_dataset_category = f\"{IMAGES_DIR}{dataset_category}/\"\n",
    "        full_label_dir = f\"{IMAGES_DIR}{dataset_category}/\"\n",
    "        for filename in os.listdir(full_dataset_category):\n",
    "            if not filename.lower().endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "                continue\n",
    "\n",
    "            img_path = os.path.join(full_dataset_category, filename)\n",
    "            print(img_path)\n",
    "            lbl_path = os.path.join(full_label_dir, filename.replace(\".jpg\", \".txt\").replace(\".png\", \".txt\"))\n",
    "\n",
    "            if not os.path.exists(lbl_path):\n",
    "                print(f\"âš  No label file found for: {filename}\")\n",
    "                continue\n",
    "\n",
    "            # Load image\n",
    "            img = cv2.imread(img_path)\n",
    "            img_h, img_w = img.shape[:2]\n",
    "\n",
    "            # Load YOLO labels\n",
    "            boxes = []\n",
    "            with open(lbl_path, \"r\") as f:\n",
    "                lines = f.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 5:\n",
    "                    print(f\"âš  Skipping invalid label line in {lbl_path}: {line}\")\n",
    "                    continue\n",
    "\n",
    "                cls_id = int(parts[0])\n",
    "                cx, cy, w, h = map(float, parts[1:5])\n",
    "\n",
    "                x1, y1, x2, y2 = yolo_to_xyxy((cx, cy, w, h), img_w, img_h)\n",
    "\n",
    "                boxes.append({\n",
    "                    \"class_id\": cls_id,\n",
    "                    \"class_name\": class_names[cls_id] if cls_id < len(class_names) else \"UNKNOWN\",\n",
    "                    \"bbox_normalized\": [cx, cy, w, h],\n",
    "                    \"bbox_xyxy\": [x1, y1, x2, y2]\n",
    "                })\n",
    "                print(boxes)\n",
    "\n",
    "            dataset.append({\n",
    "                \"image_path\": img_path,\n",
    "                \"image\": img,\n",
    "                \"boxes\": boxes\n",
    "            })\n",
    "\n",
    "        print(f\"ğŸ“¦ Loaded {len(dataset)} images with labels.\")\n",
    "        return dataset\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Example Usage\n",
    "# ------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    data = load_ocr_dataset()\n",
    "    print(data[0])  # Show first dataset entry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2566a715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"
     ]
    }
   ],
   "source": [
    "class_names = load_class_names(CLASSES_FILE)\n",
    "print(class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
