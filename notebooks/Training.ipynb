{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134d078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation of some requirements\n",
    "# %pip install albumentations\n",
    "# %pip install ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d9a18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle dataset download\n",
    "# !kaggle datasets download juanthomaswijaya/indonesian-license-plate-dataset --path data --unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7438ba5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch # If you use PyTorch later for training\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Choose an integer seed (e.g., 42, 0, or any number)\n",
    "SEED = 42\n",
    "\n",
    "def set_all_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    # If you use PyTorch, uncomment these:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_all_seeds(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8793253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryan_santosa/conda_envs/torchgpu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing directory: test\n",
      "Processing directory: train\n",
      "Processing directory: val\n",
      "‚úÖ Data augmentation complete!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import albumentations as A\n",
    "from glob import glob\n",
    "\n",
    "# Paths\n",
    "INPUT_DATA_ROOT = \"../data/raw/Indonesian License Plate Dataset/\"\n",
    "\n",
    "input_images = os.path.join(INPUT_DATA_ROOT, \"images\")\n",
    "input_labels = os.path.join(INPUT_DATA_ROOT, \"labels\")\n",
    "\n",
    "# OUTPUT Paths (A new folder to store the augmented data, usually outside of 'raw')\n",
    "OUTPUT_DATA_ROOT = \"../data/processed/\"\n",
    "\n",
    "output_images = os.path.join(OUTPUT_DATA_ROOT, \"images\")\n",
    "output_labels = os.path.join(OUTPUT_DATA_ROOT, \"labels\")\n",
    "\n",
    "# --- END: Updated Paths ---\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(output_images, exist_ok=True)\n",
    "os.makedirs(output_labels, exist_ok=True)\n",
    "\n",
    "# Define augmentation pipeline\n",
    "transform = A.Compose([\n",
    "    A.Rotate(limit=5, p=0.5),\n",
    "    A.Perspective(scale=(0.02, 0.05)),\n",
    "    A.RandomBrightnessContrast(p=0.5),\n",
    "    A.MotionBlur(blur_limit=5, p=0.3),\n",
    "    A.RandomScale(scale_limit=0.2, p=0.5),\n",
    "],\n",
    "bbox_params=A.BboxParams(\n",
    "    format='yolo', \n",
    "    label_fields=['class_labels'],\n",
    "    # Add clip=True to ensure all coordinates are strictly within [0.0, 1.0]\n",
    "    clip=True,\n",
    "    min_area=1.0\n",
    "))\n",
    "\n",
    "# Loop through all images\n",
    "for subdir in os.listdir(input_images):\n",
    "    # Construct the full paths for the current subdirectory\n",
    "    current_input_image_dir = os.path.join(input_images, subdir)\n",
    "    current_input_label_dir = os.path.join(input_labels, subdir)\n",
    "\n",
    "    # üõë FIX 2: Create the corresponding output subdirectories\n",
    "    current_output_image_dir = os.path.join(output_images, subdir)\n",
    "    current_output_label_dir = os.path.join(output_labels, subdir)\n",
    "\n",
    "    os.makedirs(current_output_image_dir, exist_ok=True)\n",
    "    os.makedirs(current_output_label_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"Processing directory: {subdir}\")\n",
    "\n",
    "    # Loop through all images in the subdirectory\n",
    "    # Using os.path.join is safer than f-string concatenation for paths\n",
    "    for img_path in glob(os.path.join(current_input_image_dir, \"*.jpg\")):\n",
    "        filename = os.path.basename(img_path)\n",
    "        \n",
    "        # üõë FIX 1: Correctly construct the label path using the subdirectory path\n",
    "        label_path = os.path.join(current_input_label_dir, filename.replace(\".jpg\", \".txt\"))\n",
    "\n",
    "        # Safety Check: Ensure the label file exists before proceeding\n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"Warning: Label file not found for {filename} in {subdir}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # Read image and labels (continue with your original logic)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            print(f\"Error: Could not read image {img_path}. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        h, w, _ = image.shape # This is for context, not strictly needed by the augmentation code\n",
    "\n",
    "        # ... (rest of the label reading, augmentation, and saving logic) ...\n",
    "\n",
    "        with open(label_path, \"r\") as f:\n",
    "            labels = f.readlines()\n",
    "\n",
    "        bboxes = []\n",
    "        class_labels = []\n",
    "        for label in labels:\n",
    "            try:\n",
    "                cls, x, y, bw, bh = map(float, label.strip().split())\n",
    "                bboxes.append([x, y, bw, bh])\n",
    "                class_labels.append(int(cls))\n",
    "            except Exception as e:\n",
    "                 # Added error handling for malformed lines\n",
    "                print(f\"Error processing label line in {label_path}: {e}. Skipping image.\")\n",
    "                continue\n",
    "\n",
    "        # Apply transformation\n",
    "        transformed = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "\n",
    "        aug_img = transformed['image']\n",
    "        aug_bboxes = transformed['bboxes']\n",
    "        aug_labels = transformed['class_labels']\n",
    "\n",
    "        # üõë FIX 2 (Final Save): Save augmented files into the correct OUTPUT subdirectory\n",
    "        \n",
    "        # Adding 'aug_' prefix is good practice to distinguish augmented files\n",
    "        aug_filename = f\"aug_{filename}\" \n",
    "\n",
    "        # Save augmented image\n",
    "        save_path_img = os.path.join(current_output_image_dir, aug_filename)\n",
    "        cv2.imwrite(save_path_img, aug_img)\n",
    "\n",
    "        # Save updated YOLO labels\n",
    "        save_path_lbl = os.path.join(current_output_label_dir, aug_filename.replace(\".jpg\", \".txt\"))\n",
    "        with open(save_path_lbl, \"w\") as f:\n",
    "            for cls, bbox in zip(aug_labels, aug_bboxes):\n",
    "                f.write(f\"{cls} {' '.join(map(str, bbox))}\\n\")\n",
    "\n",
    "print(\"‚úÖ Data augmentation complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "608491d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO version: 1\n",
      "CUDA available: True\n",
      "Device being used: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "print(\"YOLO version:\", YOLO._version)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device being used:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8d23445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model device (PyTorch check): cuda:0\n",
      "Starting YOLOv8 training...\n",
      "New https://pypi.org/project/ultralytics/8.3.233 available üòÉ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.223 üöÄ Python-3.10.19 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=6, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=../data.yaml, degrees=0.0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=40, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=lp_detection_v3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/notebooks/runs/detect/lp_detection_v3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 1.7¬±0.1 ms, read: 148.9¬±62.8 MB/s, size: 2233.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/data/processed/labels/train... 800 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 800/800 349.4it/s 2.3s0.2s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/data/processed/labels/train.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 3.4¬±1.6 ms, read: 112.6¬±21.1 MB/s, size: 1689.3 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/data/processed/labels/val... 100 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 100/100 268.8it/s 0.4s1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/data/processed/labels/val.cache\n",
      "Plotting labels to /mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/notebooks/runs/detect/lp_detection_v3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.000515625), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/notebooks/runs/detect/lp_detection_v3\u001b[0m\n",
      "Starting training for 40 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/40     0.854G      1.496      2.545      1.142          2        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 6.8it/s 19.6s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 5.7it/s 1.6s0.2s\n",
      "                   all        100        179       0.95      0.857      0.959      0.527\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/40     0.938G      1.368      1.471      1.065          9        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 8.2it/s 16.3s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 5.2it/s 1.7s0.2s\n",
      "                   all        100        179      0.871      0.828      0.917      0.562\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/40     0.938G       1.34      1.236      1.079          3        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 8.0it/s 16.6s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.3it/s 1.1s0.1s\n",
      "                   all        100        179      0.958      0.886      0.958      0.585\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/40     0.938G       1.34      1.131      1.048          8        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.9it/s 16.9s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.0it/s 1.1s0.1s\n",
      "                   all        100        179      0.931      0.911      0.963      0.611\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/40     0.938G       1.33      1.043      1.042          7        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 8.3it/s 16.1s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.3it/s 1.1s0.1s\n",
      "                   all        100        179       0.96      0.948      0.979       0.59\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/40     0.938G      1.303     0.9949      1.039          6        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.6it/s 17.7s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.3it/s 1.1s0.1s\n",
      "                   all        100        179      0.967      0.933      0.978      0.649\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/40     0.938G      1.329     0.9101      1.073          7        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.8it/s 17.2s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.6it/s 1.1s0.1s\n",
      "                   all        100        179      0.942      0.901      0.955      0.626\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/40     0.938G      1.285     0.8599      1.032          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 8.3it/s 16.1s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 9.1it/s 1.0s0.1s\n",
      "                   all        100        179       0.96       0.95      0.983      0.578\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/40     0.938G      1.207     0.8148      1.002          3        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.9it/s 17.1s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 7.9it/s 1.1s0.1s\n",
      "                   all        100        179      0.966      0.962      0.989       0.65\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/40     0.938G       1.22     0.7847       1.01          3        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 8.4it/s 15.9s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.6it/s 1.1s0.1s\n",
      "                   all        100        179      0.945      0.962      0.986       0.63\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/40     0.938G      1.205     0.7651     0.9973          3        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.9it/s 16.9s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 7.9it/s 1.1s0.1s\n",
      "                   all        100        179      0.971      0.945      0.986      0.682\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/40     0.938G      1.202     0.7394     0.9956          5        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.1it/s 19.0s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.2it/s 1.1s0.1s\n",
      "                   all        100        179      0.929      0.952      0.976      0.643\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/40     0.938G      1.195     0.7795     0.9971          6        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 8.1it/s 16.6s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 7.5it/s 1.2s0.1s\n",
      "                   all        100        179       0.95      0.961      0.981      0.674\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/40     0.939G      1.191     0.7281     0.9933         14        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.7it/s 17.4s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 7.6it/s 1.2s0.1s\n",
      "                   all        100        179      0.964      0.944      0.987      0.676\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/40     0.939G        1.2     0.7151      1.006          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 8.1it/s 16.6s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 7.5it/s 1.2s0.2s\n",
      "                   all        100        179      0.971      0.951      0.989      0.656\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/40     0.939G      1.161     0.7108     0.9951         10        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 8.0it/s 16.7s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.6it/s 1.0s0.1s\n",
      "                   all        100        179      0.949      0.945      0.988      0.601\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/40     0.939G      1.169     0.6936     0.9932          5        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.9it/s 17.0s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.3it/s 1.1s0.1s\n",
      "                   all        100        179       0.95      0.954      0.978      0.637\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/40     0.939G      1.148     0.6711     0.9723          2        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.7it/s 17.3s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 7.4it/s 1.2s0.2s\n",
      "                   all        100        179      0.945      0.966      0.986      0.658\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      19/40     0.939G      1.148     0.6676     0.9895          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.8it/s 17.3s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.2it/s 1.1s0.1s\n",
      "                   all        100        179      0.956      0.978      0.991      0.652\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      20/40     0.939G      1.151     0.6642     0.9794          8        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.7it/s 17.5s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.2it/s 1.1s0.1s\n",
      "                   all        100        179      0.973      0.944      0.987      0.672\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      21/40     0.939G      1.149     0.6653     0.9829          3        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 8.0it/s 16.8s0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 7.9it/s 1.1s0.1s\n",
      "                   all        100        179      0.967       0.95      0.991      0.679\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      22/40     0.939G      1.149     0.6544     0.9818         14        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.6it/s 17.6s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.5it/s 1.1s0.1s\n",
      "                   all        100        179      0.971      0.966      0.992      0.689\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      23/40     0.939G      1.118     0.6196     0.9744          8        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.8it/s 17.2s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 7.5it/s 1.2s0.2s\n",
      "                   all        100        179      0.957      0.961      0.991      0.708\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      24/40     0.939G      1.112     0.6166     0.9704          6        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.9it/s 16.9s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 9.0it/s 1.0s0.1s\n",
      "                   all        100        179      0.956       0.98       0.99      0.652\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      25/40     0.939G      1.078      0.591      0.956          5        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.1it/s 18.9s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 7.7it/s 1.2s0.2s\n",
      "                   all        100        179      0.966      0.962      0.992      0.696\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      26/40     0.939G      1.101     0.6059     0.9652          2        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 6.6it/s 20.2s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 9.0it/s 1.0s0.1s\n",
      "                   all        100        179      0.962      0.961      0.991      0.681\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      27/40     0.939G       1.09      0.616     0.9594          5        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 8.3it/s 16.1s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 9.6it/s 0.9s0.1s\n",
      "                   all        100        179      0.955      0.972      0.987      0.707\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      28/40     0.939G      1.096     0.5995     0.9666          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.0it/s 19.1s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.7it/s 1.0s0.1s\n",
      "                   all        100        179      0.966       0.95      0.991      0.701\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      29/40     0.939G      1.067     0.5835     0.9418          2        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.8it/s 17.3s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 6.8it/s 1.3s0.2s\n",
      "                   all        100        179       0.95      0.962       0.99      0.694\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      30/40     0.939G      1.045     0.5659     0.9368          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.8it/s 17.1s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.6it/s 1.0s0.1s\n",
      "                   all        100        179      0.935      0.978      0.988      0.689\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      31/40     0.939G      1.016     0.5396     0.9561          3        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 6.8it/s 19.8s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.9it/s 1.0s0.1s\n",
      "                   all        100        179       0.96      0.978      0.988        0.7\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      32/40     0.939G      1.003     0.5351     0.9387          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 8.0it/s 16.7s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 7.7it/s 1.2s0.2s\n",
      "                   all        100        179      0.962      0.966       0.99      0.679\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      33/40     0.939G     0.9965     0.5308     0.9563          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.7it/s 17.3s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 7.8it/s 1.2s0.1s\n",
      "                   all        100        179      0.935      0.978      0.989      0.711\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      34/40     0.939G      1.002     0.5215     0.9528          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.8it/s 17.3s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.4it/s 1.1s0.1s\n",
      "                   all        100        179       0.94      0.966       0.99      0.646\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      35/40     0.939G      0.998     0.5215     0.9517          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 8.3it/s 16.2s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 9.8it/s 0.9s0.1s\n",
      "                   all        100        179      0.956      0.972      0.992      0.694\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      36/40     0.939G     0.9868     0.5059     0.9535          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.9it/s 16.9s0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.5it/s 1.1s0.1s\n",
      "                   all        100        179      0.951      0.989      0.992      0.695\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      37/40     0.939G     0.9526      0.502      0.937          4        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.9it/s 16.9s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 7.8it/s 1.2s0.1s\n",
      "                   all        100        179      0.951      0.979      0.992      0.698\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      38/40     0.939G     0.9644     0.4979     0.9385          3        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.9it/s 16.9s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.6it/s 1.0s0.1s\n",
      "                   all        100        179      0.972      0.957      0.992      0.716\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      39/40     0.939G     0.9423     0.4819     0.9148          2        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.9it/s 17.1s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 8.1it/s 1.1s0.1s\n",
      "                   all        100        179       0.98      0.961      0.993      0.729\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      40/40     0.939G     0.9355     0.4805     0.9361          3        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 134/134 7.8it/s 17.1s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 7.7it/s 1.2s0.1s\n",
      "                   all        100        179      0.966      0.961      0.993      0.711\n",
      "\n",
      "40 epochs completed in 0.211 hours.\n",
      "Optimizer stripped from /mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/notebooks/runs/detect/lp_detection_v3/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from /mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/notebooks/runs/detect/lp_detection_v3/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/notebooks/runs/detect/lp_detection_v3/weights/best.pt...\n",
      "Ultralytics 8.3.223 üöÄ Python-3.10.19 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 9/9 5.5it/s 1.6s0.2s\n",
      "                   all        100        179      0.981      0.961      0.993      0.728\n",
      "Speed: 0.3ms preprocess, 3.6ms inference, 0.0ms loss, 2.7ms postprocess per image\n",
      "Results saved to \u001b[1m/mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/notebooks/runs/detect/lp_detection_v3\u001b[0m\n",
      "Training complete. Results saved in runs/detect/lp_detection_v3\n"
     ]
    }
   ],
   "source": [
    "DATA_YAML_PATH = '../data.yaml'\n",
    "\n",
    "target_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Load a pre-trained YOLOv8-nano model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "model.model.to(target_device) \n",
    "\n",
    "# 3. Verify the change (using the reliable check)\n",
    "first_param_device = next(model.model.parameters()).device\n",
    "\n",
    "print(\"‚úÖ Model device (PyTorch check):\", first_param_device)\n",
    "\n",
    "print(\"Starting YOLOv8 training...\")\n",
    "results = model.train(\n",
    "    data=DATA_YAML_PATH,     # Path to the data configuration file\n",
    "    epochs=40,              # Number of epochs (adjust as needed)\n",
    "    imgsz=640,               # Input image size (standard for YOLO)\n",
    "    batch=6,                # Batch size (reduce if you run out of GPU memory)\n",
    "    name='lp_detection_v3',  # Name for the results folder in 'runs/detect\n",
    "    # Optional: use the 'freeze' argument if you want to freeze the backbone \n",
    "    amp=True,\n",
    "    device=0,  \n",
    "    lr0=1e-3\n",
    ")\n",
    "\n",
    "print(\"Training complete. Results saved in runs/detect/lp_detection_v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "874142b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.223 üöÄ Python-3.10.19 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 6144MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 1.7¬±0.1 ms, read: 134.6¬±15.8 MB/s, size: 1701.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/data/processed/labels/val.cache... 100 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 100/100 119.5Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7/7 1.5it/s 4.7s0.4ss\n",
      "                   all        100        179       0.98      0.961      0.993      0.729\n",
      "Speed: 8.8ms preprocess, 7.5ms inference, 0.0ms loss, 3.2ms postprocess per image\n",
      "Results saved to \u001b[1m/mnt/d/Uni/Semester 5/Computer Vision/Plate-Recognition/notebooks/runs/detect/val4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "evaluate = model.val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "077613e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns: ['epoch', 'time', 'train/box_loss', 'train/cls_loss', 'train/dfl_loss', 'metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)', 'val/box_loss', 'val/cls_loss', 'val/dfl_loss', 'lr/pg0', 'lr/pg1', 'lr/pg2']\n",
      "                              0          1          2          3          4\n",
      "epoch                  1.000000   2.000000   3.000000   4.000000   5.000000\n",
      "time                  21.254500  39.879400  58.225500  76.807000  94.631500\n",
      "train/box_loss         1.495820   1.367810   1.340050   1.340440   1.330070\n",
      "train/cls_loss         2.545190   1.470670   1.236440   1.130550   1.042730\n",
      "train/dfl_loss         1.142130   1.064740   1.078760   1.048280   1.042080\n",
      "metrics/precision(B)   0.950420   0.870700   0.957740   0.931430   0.960400\n",
      "metrics/recall(B)      0.856820   0.827650   0.886350   0.910630   0.948410\n",
      "metrics/mAP50(B)       0.959420   0.916750   0.957720   0.963430   0.979110\n",
      "metrics/mAP50-95(B)    0.527450   0.561900   0.585330   0.611060   0.589950\n",
      "val/box_loss           1.320000   1.216000   1.231600   1.129700   1.230540\n",
      "val/cls_loss           1.784700   1.088660   1.026530   0.922860   0.798160\n",
      "val/dfl_loss           1.103980   1.051530   1.051450   1.015830   1.034730\n",
      "lr/pg0                 0.000662   0.001295   0.001896   0.001852   0.001802\n",
      "lr/pg1                 0.000662   0.001295   0.001896   0.001852   0.001802\n",
      "lr/pg2                 0.000662   0.001295   0.001896   0.001852   0.001802\n",
      "Using metric column: metrics/mAP50(B)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv(\"runs/detect/lp_detection_v3/results.csv\")\n",
    "\n",
    "# Inspect columns and head to see what's actually in the CSV\n",
    "print(\"columns:\", df.columns.tolist())\n",
    "print(df.head().T)\n",
    "\n",
    "# Pick the mAP column robustly\n",
    "map_cols = [c for c in df.columns if \"mAP50\" in c or \"map50\" in c.lower() or \"mAP\" in c]\n",
    "if not map_cols:\n",
    "    raise ValueError(\"No mAP column found in results.csv. Columns: \" + \", \".join(df.columns))\n",
    "col = map_cols[0]\n",
    "print(\"Using metric column:\", col)\n",
    "\n",
    "# Convert to numeric (coerce errors -> NaN) and warn if any NaNs\n",
    "y = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "if y.isnull().any():\n",
    "    print(\"Warning: some values in the metric column could not be converted to numeric and became NaN\")\n",
    "\n",
    "# Use epoch column if present else use dataframe index\n",
    "x = df[\"epoch\"] if \"epoch\" in df.columns else df.index\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(x, y, marker=\"o\")\n",
    "plt.xlabel(\"epoch\" if \"epoch\" in df.columns else \"index\")\n",
    "plt.ylabel(col)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "636a7695",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     27\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_class_dir, filename)\n\u001b[0;32m---> 28\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "YOLO_MODEL_PATH = \"./runs/detect/lp_detection_v3/weights/best.pt\"\n",
    "INPUT_DIR = \"../data/processed/images/\"\n",
    "OUTPUT_DIR = \"../data/cropped/\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "model = YOLO(YOLO_MODEL_PATH)\n",
    "\n",
    "mapping = {}\n",
    "\n",
    "for class_name in os.listdir(INPUT_DIR):\n",
    "    input_class_dir = os.path.join(INPUT_DIR, class_name)\n",
    "    output_class_dir = os.path.join(OUTPUT_DIR, class_name)\n",
    "\n",
    "    # Create output directory for this class\n",
    "    os.makedirs(output_class_dir, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_class_dir):\n",
    "        if not filename.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
    "            continue\n",
    "\n",
    "        img_path = os.path.join(input_class_dir, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        results = model.predict(img, verbose=False)\n",
    "        boxes = results[0].boxes\n",
    "\n",
    "        bboxes = boxes.xyxy.cpu().numpy()\n",
    "        confs = boxes.conf.cpu().numpy()\n",
    "\n",
    "        cropped_entries = []  # reset for each image\n",
    "\n",
    "        for i, (x1, y1, x2, y2) in enumerate(bboxes[:, :4]):\n",
    "            conf = confs[i]\n",
    "            if conf < 0.5:\n",
    "                continue\n",
    "\n",
    "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
    "            crop = img[y1:y2, x1:x2]\n",
    "\n",
    "            crop_name = f\"{os.path.splitext(filename)[0]}_plate{i+1}.jpg\"\n",
    "            crop_path = os.path.join(output_class_dir, crop_name)\n",
    "\n",
    "            cv2.imwrite(crop_path, crop)\n",
    "\n",
    "            cropped_entries.append({\n",
    "                \"crop\": crop_name,\n",
    "                \"confidence\": float(conf),\n",
    "                \"bbox\": [x1, y1, x2, y2]\n",
    "            })\n",
    "\n",
    "        if cropped_entries:\n",
    "            mapping[filename] = cropped_entries\n",
    "\n",
    "# Save mapping once at the end\n",
    "mapping_file = os.path.join(OUTPUT_DIR, \"detections.json\")\n",
    "with open(mapping_file, \"w\") as f:\n",
    "    json.dump(mapping, f, indent=4)\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "544d7a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_for_ocr(img, target_h=32, target_w=128):\n",
    "    # 1. Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # 2. (Optional) LIGHT denoise ‚Äî avoid distortion\n",
    "    # denoised = cv2.fastNlMeansDenoising(gray, h=10)\n",
    "\n",
    "    # 3. Resize to target OCR input size\n",
    "    resized = cv2.resize(gray, (target_w, target_h), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    return resized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22085bf",
   "metadata": {},
   "source": [
    "<h1> Preprocess OCR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d401922c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/cropped/val\n",
      "['detections.json', 'test', 'train', 'val']\n",
      "detections.json\n",
      "../data/cropped/detections.json\n",
      "['detections.json', 'test', 'train', 'val']\n",
      "test\n",
      "../data/cropped/test\n",
      "['detections.json', 'test', 'train', 'val']\n",
      "train\n",
      "../data/cropped/train\n",
      "['detections.json', 'test', 'train', 'val']\n",
      "val\n",
      "\n",
      "üéâ All license plates have been preprocessed for OCR!\n"
     ]
    }
   ],
   "source": [
    "cropped_OCR_dir = \"../data/cropped/\"\n",
    "output_OCR_dir = \"../data/trainOCR/images/\"\n",
    "\n",
    "os.makedirs(output_OCR_dir, exist_ok=True)\n",
    "\n",
    "for dataset_category in os.listdir(cropped_OCR_dir):\n",
    "    print(full_dataset_category)\n",
    "    print(os.listdir(cropped_OCR_dir))\n",
    "    print(dataset_category)\n",
    "    full_dataset_category = os.path.join(cropped_OCR_dir, dataset_category)\n",
    "    output_category_dir = os.path.join(output_OCR_dir, dataset_category)\n",
    "    if not os.path.isdir(full_dataset_category):\n",
    "        continue\n",
    "    os.makedirs(output_category_dir, exist_ok=True)\n",
    "    for filename in os.listdir(full_dataset_category):\n",
    "        full_cropped_image_path = os.path.join(full_dataset_category, filename)\n",
    "        full_output_image_path = os.path.join(output_category_dir, filename)\n",
    "\n",
    "        # --- Load image ---\n",
    "        img = cv2.imread(full_cropped_image_path)\n",
    "        if img is None:\n",
    "            print(f\"‚ùå Cannot read {full_cropped_image_path}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # --- Preprocess the image ---\n",
    "        processed_img = preprocess_for_ocr(img)\n",
    "\n",
    "        # --- IMPORTANT: Convert to uint8 before saving ---\n",
    "        processed_img_uint8 = (processed_img * 255).astype(\"uint8\")\n",
    "\n",
    "        # --- Save the processed image ---\n",
    "        cv2.imwrite(full_output_image_path, processed_img_uint8)\n",
    "\n",
    "print(\"\\nüéâ All license plates have been preprocessed for OCR!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2355154",
   "metadata": {},
   "source": [
    "<h1> Training OCR recognition model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe5cbdb",
   "metadata": {},
   "source": [
    "<h1>OCR belom bener sama sekali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b51e4b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ocr_crnn_train.py\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from typing import List\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Optional fast Levenshtein\n",
    "try:\n",
    "    import editdistance\n",
    "    def levenshtein(a,b): return editdistance.eval(a,b)\n",
    "except Exception:\n",
    "    def levenshtein(a, b):\n",
    "        # simple DP implementation\n",
    "        la, lb = len(a), len(b)\n",
    "        dp = list(range(lb+1))\n",
    "        for i in range(1, la+1):\n",
    "            prev = dp[:]\n",
    "            dp[0] = i\n",
    "            for j in range(1, lb+1):\n",
    "                dp[j] = min(prev[j] + 1, dp[j-1] + 1, prev[j-1] + (0 if a[i-1]==b[j-1] else 1))\n",
    "        return dp[lb]\n",
    "\n",
    "# -------------------------\n",
    "# User config / paths\n",
    "# -------------------------\n",
    "ROOT = \"../data/raw/Indonesian License Plate Recognition Dataset\"\n",
    "TARGET_H = 32\n",
    "TARGET_W = 128   # fixed width (for batching). You can increase if your plates are wide.\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BEST_MODEL_PATH = \"./OCR/best_crnn.pth\"\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Utilities: read classes.names\n",
    "# -------------------------\n",
    "def load_classes(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = [l.rstrip(\"\\n\") for l in f if l.strip()!=\"\"]\n",
    "    return lines\n",
    "\n",
    "# -------------------------\n",
    "# Build transcription from YOLO character labels\n",
    "# label line format: \"<class_id> <cx> <cy> <w> <h>\"\n",
    "# We sort by cx to get left-to-right reading order.\n",
    "# -------------------------\n",
    "def parse_label_file(label_path, class_names):\n",
    "    chars = []\n",
    "    try:\n",
    "        with open(label_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "                cls = int(parts[0])\n",
    "                cx = float(parts[1])\n",
    "                #cy = float(parts[2])\n",
    "                #w = float(parts[3]); h = float(parts[4])\n",
    "                chars.append((cx, class_names[cls] if cls < len(class_names) else \"?\"))\n",
    "    except FileNotFoundError:\n",
    "        return \"\"\n",
    "    # sort by cx and join\n",
    "    chars.sort(key=lambda x: x[0])\n",
    "    transcription = \"\".join([c for _, c in chars])\n",
    "    return transcription\n",
    "\n",
    "# -------------------------\n",
    "# Dataset\n",
    "# -------------------------\n",
    "class PlateOCRDataset(Dataset):\n",
    "\n",
    "    def __init__(self, root: str, split: str, class_names, transform=None):\n",
    "        assert split in (\"train\", \"val\", \"test\"), \"split must be one of 'train','val','test'\"\n",
    "\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.img_dir = os.path.join(root, \"images\", split)\n",
    "        self.lbl_dir = os.path.join(root, \"labels\", split)\n",
    "        if not os.path.isdir(self.img_dir):\n",
    "            raise FileNotFoundError(f\"Images directory not found: {self.img_dir}\")\n",
    "        if not os.path.isdir(self.lbl_dir):\n",
    "            # It's OK to allow missing label files per image, but folder should exist\n",
    "            raise FileNotFoundError(f\"Labels directory not found: {self.lbl_dir}\")\n",
    "\n",
    "        # collect image files\n",
    "        self.files = sorted([\n",
    "            f for f in os.listdir(self.img_dir)\n",
    "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "        ])\n",
    "\n",
    "        self.class_names = class_names\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def _build_label_path(self, image_filename):\n",
    "        name, _ = os.path.splitext(image_filename)\n",
    "        return os.path.join(self.lbl_dir, name + \".txt\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.files[idx]\n",
    "        img_path = os.path.join(self.img_dir, filename)\n",
    "        lbl_path = self._build_label_path(filename)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            raise RuntimeError(f\"Failed to read image: {img_path}\")\n",
    "\n",
    "        # --- preprocess (still numpy 2D array H√óW) ---\n",
    "        if self.transform is not None:\n",
    "            img_proc = self.transform(img)\n",
    "        else:\n",
    "            img_proc = img\n",
    "\n",
    "        # --- convert to tensor CHW ---\n",
    "        # img_proc shape: (H, W) ‚Üí (1, H, W)\n",
    "        img_tensor = torch.from_numpy(img_proc)\n",
    "        img_tensor = img_tensor.unsqueeze(0) \n",
    "        img_tensor = img_tensor.float() / 255.0\n",
    "\n",
    "        # Build transcription string\n",
    "        transcription = \"\"\n",
    "        if os.path.exists(lbl_path):\n",
    "            chars = []\n",
    "            with open(lbl_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    parts = line.split()\n",
    "                    if len(parts) < 5:\n",
    "                        continue\n",
    "                    cls_id = int(parts[0])\n",
    "                    cx = float(parts[1])\n",
    "                    char = self.class_names[cls_id]\n",
    "                    chars.append((cx, char))\n",
    "            chars.sort(key=lambda x: x[0])\n",
    "            transcription = \"\".join([c for _, c in chars])\n",
    "\n",
    "        return img_tensor, transcription\n",
    "    \n",
    "# -------------------------\n",
    "# Collate: convert batch of (tensor, str) into\n",
    "# tensors for CTC: inputs (B, C, H, W), targets concatenated, target_lengths, input_lengths\n",
    "# We'll encode characters to indices using char2idx mapping.\n",
    "# -------------------------\n",
    "def collate_fn(batch, char2idx):\n",
    "    images = []\n",
    "    labels = []\n",
    "    texts = []\n",
    "\n",
    "    for img, text in batch:\n",
    "        images.append(img)              # img is already (1,H,W)\n",
    "        seq = [char2idx[c] for c in text]\n",
    "        labels.append(seq)\n",
    "        texts.append(text)\n",
    "\n",
    "    # Images are already 1xHxW, so we just stack\n",
    "    images = torch.stack(images).float()\n",
    "\n",
    "    flat_labels = [x for seq in labels for x in seq]\n",
    "    targets = torch.tensor(flat_labels, dtype=torch.long)\n",
    "\n",
    "    target_lengths = torch.tensor([len(seq) for seq in labels], dtype=torch.long)\n",
    "\n",
    "    input_lengths = torch.full(\n",
    "        size=(len(batch),),\n",
    "        fill_value=images.shape[-1] // 4,\n",
    "        dtype=torch.long\n",
    "    )\n",
    "\n",
    "    return images, targets, target_lengths,texts\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Small CRNN model\n",
    "# -------------------------\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_chars):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1,64,3,1,1), nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "\n",
    "            nn.Conv2d(64,128,3,1,1), nn.ReLU(),\n",
    "            nn.MaxPool2d((2,1)),\n",
    "\n",
    "            nn.Conv2d(128,256,3,1,1), nn.ReLU(),\n",
    "            nn.MaxPool2d((2,1)),\n",
    "\n",
    "            nn.Conv2d(256,512,3,1,1), nn.ReLU(),\n",
    "            nn.MaxPool2d((2,1)),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d((1,None))\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(512, 256)\n",
    "        self.relu   = nn.ReLU()\n",
    "        self.gru    = nn.GRU(256, 256, num_layers=2, bidirectional=True, batch_first=True)\n",
    "\n",
    "        self.embedding = nn.Linear(512, num_chars+1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv = self.cnn(x)\n",
    "        conv = conv.squeeze(2)        # (B,512,W)\n",
    "        conv = conv.permute(0,2,1)    # (B,W,512)\n",
    "\n",
    "        x = self.relu(self.linear(conv))\n",
    "        x, _ = self.gru(x)\n",
    "        logits = self.embedding(x)\n",
    "\n",
    "        return logits.permute(1,0,2)  # (T,B,C)\n",
    "\n",
    "# -------------------------\n",
    "# Greedy decode for logits -> string\n",
    "# -------------------------\n",
    "def greedy_decode(logits, idx2char):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    max_idx = torch.argmax(probs, dim=-1)\n",
    "\n",
    "    T, B = max_idx.shape\n",
    "    blank = len(idx2char)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for b in range(B):\n",
    "        pred = []\n",
    "        last = -1\n",
    "        for t in range(T):\n",
    "            i = max_idx[t,b].item()\n",
    "            if i != blank and i != last:\n",
    "                pred.append(idx2char[i])\n",
    "            last = i\n",
    "        results.append(\"\".join(pred))\n",
    "\n",
    "    return results\n",
    "# -------------------------\n",
    "# CER computation\n",
    "# -------------------------\n",
    "def cer(preds: List[str], targets: List[str]):\n",
    "    total_ed = 0\n",
    "    total_chars = 0\n",
    "    for p,t in zip(preds, targets):\n",
    "        total_ed += levenshtein(p, t)\n",
    "        total_chars += max(1, len(t))\n",
    "    return total_ed / total_chars\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dda26f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch shape: torch.Size([64, 1, 32, 128])\n",
      "\n",
      "DEBUG: val batch 0 shapes -> logits: torch.Size([64, 64, 37]), targets.len=470, target_lengths.shape=torch.Size([64]), texts.len=64\n",
      "DEBUG: sum(target_lengths) = 470  targets.numel() = 470\n",
      " SAMP 0: GT='B9079FXX'  PRED=''\n",
      " SAMP 1: GT='B1950NZG'  PRED=''\n",
      " SAMP 2: GT='H9019YC'  PRED=''\n",
      " SAMP 3: GT='G7D'  PRED=''\n",
      " SAMP 4: GT='AD1067DC'  PRED=''\n",
      " SAMP 5: GT='L2905NC'  PRED=''\n",
      " SAMP 6: GT='H8011EQ'  PRED=''\n",
      " SAMP 7: GT='B1986VMQ'  PRED=''\n",
      "DEBUG: avg blank prob = 0.9555, avg char prob = 0.0012\n",
      "DEBUG: recon[0:4] from targets: ['B9079FXX', 'B1950NZG', 'H9019YC', 'G7D']\n",
      "DEBUG_VAL_CER (first-batch): 1.0\n",
      "\n",
      "DEBUG: val batch 0 shapes -> logits: torch.Size([64, 64, 37]), targets.len=470, target_lengths.shape=torch.Size([64]), texts.len=64\n",
      "DEBUG: sum(target_lengths) = 470  targets.numel() = 470\n",
      " SAMP 0: GT='B9079FXX'  PRED=''\n",
      " SAMP 1: GT='B1950NZG'  PRED=''\n",
      " SAMP 2: GT='H9019YC'  PRED=''\n",
      " SAMP 3: GT='G7D'  PRED=''\n",
      " SAMP 4: GT='AD1067DC'  PRED=''\n",
      " SAMP 5: GT='L2905NC'  PRED=''\n",
      " SAMP 6: GT='H8011EQ'  PRED=''\n",
      " SAMP 7: GT='B1986VMQ'  PRED=''\n",
      "DEBUG: avg blank prob = 0.9350, avg char prob = 0.0018\n",
      "DEBUG: recon[0:4] from targets: ['B9079FXX', 'B1950NZG', 'H9019YC', 'G7D']\n",
      "DEBUG_VAL_CER (first-batch): 1.0\n",
      "\n",
      "DEBUG: val batch 0 shapes -> logits: torch.Size([64, 64, 37]), targets.len=470, target_lengths.shape=torch.Size([64]), texts.len=64\n",
      "DEBUG: sum(target_lengths) = 470  targets.numel() = 470\n",
      " SAMP 0: GT='B9079FXX'  PRED=''\n",
      " SAMP 1: GT='B1950NZG'  PRED=''\n",
      " SAMP 2: GT='H9019YC'  PRED=''\n",
      " SAMP 3: GT='G7D'  PRED=''\n",
      " SAMP 4: GT='AD1067DC'  PRED=''\n",
      " SAMP 5: GT='L2905NC'  PRED=''\n",
      " SAMP 6: GT='H8011EQ'  PRED=''\n",
      " SAMP 7: GT='B1986VMQ'  PRED=''\n",
      "DEBUG: avg blank prob = 0.8955, avg char prob = 0.0029\n",
      "DEBUG: recon[0:4] from targets: ['B9079FXX', 'B1950NZG', 'H9019YC', 'G7D']\n",
      "DEBUG_VAL_CER (first-batch): 1.0\n",
      "\n",
      "DEBUG: val batch 0 shapes -> logits: torch.Size([64, 64, 37]), targets.len=470, target_lengths.shape=torch.Size([64]), texts.len=64\n",
      "DEBUG: sum(target_lengths) = 470  targets.numel() = 470\n",
      " SAMP 0: GT='B9079FXX'  PRED=''\n",
      " SAMP 1: GT='B1950NZG'  PRED=''\n",
      " SAMP 2: GT='H9019YC'  PRED=''\n",
      " SAMP 3: GT='G7D'  PRED=''\n",
      " SAMP 4: GT='AD1067DC'  PRED=''\n",
      " SAMP 5: GT='L2905NC'  PRED=''\n",
      " SAMP 6: GT='H8011EQ'  PRED=''\n",
      " SAMP 7: GT='B1986VMQ'  PRED=''\n",
      "DEBUG: avg blank prob = 0.8872, avg char prob = 0.0031\n",
      "DEBUG: recon[0:4] from targets: ['B9079FXX', 'B1950NZG', 'H9019YC', 'G7D']\n",
      "DEBUG_VAL_CER (first-batch): 1.0\n",
      "\n",
      "DEBUG: val batch 0 shapes -> logits: torch.Size([64, 64, 37]), targets.len=470, target_lengths.shape=torch.Size([64]), texts.len=64\n",
      "DEBUG: sum(target_lengths) = 470  targets.numel() = 470\n",
      " SAMP 0: GT='B9079FXX'  PRED=''\n",
      " SAMP 1: GT='B1950NZG'  PRED=''\n",
      " SAMP 2: GT='H9019YC'  PRED=''\n",
      " SAMP 3: GT='G7D'  PRED=''\n",
      " SAMP 4: GT='AD1067DC'  PRED=''\n",
      " SAMP 5: GT='L2905NC'  PRED=''\n",
      " SAMP 6: GT='H8011EQ'  PRED=''\n",
      " SAMP 7: GT='B1986VMQ'  PRED=''\n",
      "DEBUG: avg blank prob = 0.8872, avg char prob = 0.0031\n",
      "DEBUG: recon[0:4] from targets: ['B9079FXX', 'B1950NZG', 'H9019YC', 'G7D']\n",
      "DEBUG_VAL_CER (first-batch): 1.0\n",
      "\n",
      "DEBUG: val batch 0 shapes -> logits: torch.Size([64, 64, 37]), targets.len=470, target_lengths.shape=torch.Size([64]), texts.len=64\n",
      "DEBUG: sum(target_lengths) = 470  targets.numel() = 470\n",
      " SAMP 0: GT='B9079FXX'  PRED=''\n",
      " SAMP 1: GT='B1950NZG'  PRED=''\n",
      " SAMP 2: GT='H9019YC'  PRED=''\n",
      " SAMP 3: GT='G7D'  PRED=''\n",
      " SAMP 4: GT='AD1067DC'  PRED=''\n",
      " SAMP 5: GT='L2905NC'  PRED=''\n",
      " SAMP 6: GT='H8011EQ'  PRED=''\n",
      " SAMP 7: GT='B1986VMQ'  PRED=''\n",
      "DEBUG: avg blank prob = 0.8850, avg char prob = 0.0032\n",
      "DEBUG: recon[0:4] from targets: ['B9079FXX', 'B1950NZG', 'H9019YC', 'G7D']\n",
      "DEBUG_VAL_CER (first-batch): 1.0\n",
      "\n",
      "DEBUG: val batch 0 shapes -> logits: torch.Size([64, 64, 37]), targets.len=470, target_lengths.shape=torch.Size([64]), texts.len=64\n",
      "DEBUG: sum(target_lengths) = 470  targets.numel() = 470\n",
      " SAMP 0: GT='B9079FXX'  PRED=''\n",
      " SAMP 1: GT='B1950NZG'  PRED=''\n",
      " SAMP 2: GT='H9019YC'  PRED=''\n",
      " SAMP 3: GT='G7D'  PRED=''\n",
      " SAMP 4: GT='AD1067DC'  PRED=''\n",
      " SAMP 5: GT='L2905NC'  PRED=''\n",
      " SAMP 6: GT='H8011EQ'  PRED=''\n",
      " SAMP 7: GT='B1986VMQ'  PRED=''\n",
      "DEBUG: avg blank prob = 0.8818, avg char prob = 0.0033\n",
      "DEBUG: recon[0:4] from targets: ['B9079FXX', 'B1950NZG', 'H9019YC', 'G7D']\n",
      "DEBUG_VAL_CER (first-batch): 1.0\n",
      "\n",
      "DEBUG: val batch 0 shapes -> logits: torch.Size([64, 64, 37]), targets.len=470, target_lengths.shape=torch.Size([64]), texts.len=64\n",
      "DEBUG: sum(target_lengths) = 470  targets.numel() = 470\n",
      " SAMP 0: GT='B9079FXX'  PRED=''\n",
      " SAMP 1: GT='B1950NZG'  PRED=''\n",
      " SAMP 2: GT='H9019YC'  PRED=''\n",
      " SAMP 3: GT='G7D'  PRED=''\n",
      " SAMP 4: GT='AD1067DC'  PRED=''\n",
      " SAMP 5: GT='L2905NC'  PRED=''\n",
      " SAMP 6: GT='H8011EQ'  PRED=''\n",
      " SAMP 7: GT='B1986VMQ'  PRED=''\n",
      "DEBUG: avg blank prob = 0.8833, avg char prob = 0.0032\n",
      "DEBUG: recon[0:4] from targets: ['B9079FXX', 'B1950NZG', 'H9019YC', 'G7D']\n",
      "DEBUG_VAL_CER (first-batch): 1.0\n",
      "\n",
      "DEBUG: val batch 0 shapes -> logits: torch.Size([64, 64, 37]), targets.len=470, target_lengths.shape=torch.Size([64]), texts.len=64\n",
      "DEBUG: sum(target_lengths) = 470  targets.numel() = 470\n",
      " SAMP 0: GT='B9079FXX'  PRED=''\n",
      " SAMP 1: GT='B1950NZG'  PRED=''\n",
      " SAMP 2: GT='H9019YC'  PRED=''\n",
      " SAMP 3: GT='G7D'  PRED=''\n",
      " SAMP 4: GT='AD1067DC'  PRED=''\n",
      " SAMP 5: GT='L2905NC'  PRED=''\n",
      " SAMP 6: GT='H8011EQ'  PRED=''\n",
      " SAMP 7: GT='B1986VMQ'  PRED=''\n",
      "DEBUG: avg blank prob = 0.8831, avg char prob = 0.0032\n",
      "DEBUG: recon[0:4] from targets: ['B9079FXX', 'B1950NZG', 'H9019YC', 'G7D']\n",
      "DEBUG_VAL_CER (first-batch): 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     42\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets, target_lengths,texts \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# images: (B,1,H,W)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;66;03m# targets: 1D tensor of length = sum(target_lengths)\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;66;03m# target_lengths: (B,)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m     \u001b[38;5;66;03m# Skip empty-batch targets (no labels in any sample)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m targets\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     50\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/conda_envs/torchgpu/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/conda_envs/torchgpu/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/conda_envs/torchgpu/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/conda_envs/torchgpu/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[62], line 116\u001b[0m, in \u001b[0;36mPlateOCRDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    113\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, filename)\n\u001b[1;32m    114\u001b[0m lbl_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_label_path(filename)\n\u001b[0;32m--> 116\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to read image: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -------------------------\n",
    "# Data & Dataloaders\n",
    "# -------------------------\n",
    "# load classes\n",
    "classes_path = os.path.join(ROOT, \"classes.names\")\n",
    "class_names = load_classes(classes_path)\n",
    "# char2idx, idx2char\n",
    "char2idx = {c:i for i,c in enumerate(class_names)}\n",
    "idx2char = {i:c for c,i in char2idx.items()}\n",
    "\n",
    "train_ds = PlateOCRDataset(root=ROOT, split=\"train\", class_names=class_names, transform=preprocess_for_ocr)\n",
    "val_ds   = PlateOCRDataset(root=ROOT, split=\"val\",   class_names=class_names, transform=preprocess_for_ocr)\n",
    "test_ds  = PlateOCRDataset(root=ROOT, split=\"test\",  class_names=class_names, transform=preprocess_for_ocr)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=lambda b: collate_fn(b, char2idx))\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda b: collate_fn(b, char2idx))\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=lambda b: collate_fn(b, char2idx))\n",
    "\n",
    "# -------------------------\n",
    "# Instantiate model, loss, optimizer\n",
    "# -------------------------\n",
    "num_chars = len(class_names)\n",
    "model = CRNN(num_chars).to(DEVICE)\n",
    "ctc_loss = nn.CTCLoss(blank=num_chars, zero_infinity=True)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for batch in train_loader:\n",
    "    images, labels, _,_ = batch\n",
    "    print(\"Image batch shape:\", images.shape)\n",
    "    break\n",
    "\n",
    "# -------------------------\n",
    "# Training loop (computes input_lengths dynamically)\n",
    "# -------------------------\n",
    "def get_input_length(logits_T):  # logits shape T x B x C -> T\n",
    "    return logits_T.size(0)\n",
    "\n",
    "best_val_cer = 1.0\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    t0 = time.time()\n",
    "    for images, targets, target_lengths,texts in train_loader:\n",
    "        # images: (B,1,H,W)\n",
    "        # targets: 1D tensor of length = sum(target_lengths)\n",
    "        # target_lengths: (B,)\n",
    "\n",
    "        # Skip empty-batch targets (no labels in any sample)\n",
    "        if targets.numel() == 0:\n",
    "            continue\n",
    "\n",
    "        images = images.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        target_lengths = target_lengths.to(DEVICE)\n",
    "\n",
    "        # forward\n",
    "        logits = model(images)            # (T, B, C)\n",
    "        T = logits.size(0)\n",
    "        B = logits.size(1)\n",
    "\n",
    "        # input_lengths = all T (model produces T timesteps for each sample)\n",
    "        input_lengths = torch.full((B,), fill_value=T, dtype=torch.long, device=DEVICE)\n",
    "\n",
    "        # compute CTC loss\n",
    "        loss = ctc_loss(logits, targets, input_lengths, target_lengths)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # validation (unchanged, but ensure you unpack correctly there too)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    gts = []\n",
    "    with torch.no_grad():\n",
    "        for images, targets, target_lengths,texts in val_loader:\n",
    "            images = images.to(DEVICE)\n",
    "            logits = model(images)  # T x B x C\n",
    "            batch_preds = greedy_decode(logits.cpu(), idx2char)\n",
    "            preds.extend(batch_preds)\n",
    "            gts.extend(texts)\n",
    "            # need original strings for gts: we can rebuild strings from targets+target_lengths or\n",
    "            # change collate to also return texts; easiest: modify collate to return texts as well.\n",
    "            # For now assume you have access to texts in the val loop (see note below)\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    gts = []\n",
    "    with torch.no_grad():\n",
    "        for batch_i, (images, targets, target_lengths, texts) in enumerate(val_loader):\n",
    "            images = images.to(DEVICE)                           # (B,1,H,W)\n",
    "            logits = model(images)                              # (T, B, C)\n",
    "            T, B, C = logits.shape\n",
    "\n",
    "            # quick checks:\n",
    "            print(f\"\\nDEBUG: val batch {batch_i} shapes -> logits: {logits.shape}, targets.len={targets.numel()}, target_lengths.shape={target_lengths.shape}, texts.len={len(texts)}\")\n",
    "\n",
    "            # 1) check that flattened targets length matches sum(target_lengths)\n",
    "            sum_tlen = int(target_lengths.sum().item()) if target_lengths.numel()>0 else 0\n",
    "            print(\"DEBUG: sum(target_lengths) =\", sum_tlen, \" targets.numel() =\", targets.numel())\n",
    "            if sum_tlen != int(targets.numel()):\n",
    "                print(\"WARNING: targets length mismatch! collate_fn flattening is wrong or order mismatch.\")\n",
    "\n",
    "            # 2) greedy decode\n",
    "            batch_preds = greedy_decode(logits.cpu(), idx2char)   # list of B strings\n",
    "            for i in range(min(8, B)):\n",
    "                print(f\" SAMP {i}: GT='{texts[i]}'  PRED='{batch_preds[i]}'\")\n",
    "\n",
    "            # 3) check blank probability mass vs char mass for the batch\n",
    "            probs = torch.softmax(logits, dim=-1)  # T x B x C\n",
    "            blank_index = len(idx2char)\n",
    "            blank_probs = probs[:, :, blank_index].mean().item()   # average prob of blank\n",
    "            char_probs = probs[:, :, :blank_index].mean().item()\n",
    "            print(f\"DEBUG: avg blank prob = {blank_probs:.4f}, avg char prob = {char_probs:.4f}\")\n",
    "\n",
    "            # 4) sanity: reconstruct ground-truth strings from targets+target_lengths to ensure match `texts`\n",
    "            recon = []\n",
    "            if targets.numel() > 0:\n",
    "                # targets is 1D tensor of ints\n",
    "                start = 0\n",
    "                for ln in target_lengths.tolist():\n",
    "                    seq = targets[start:start+ln].cpu().tolist()\n",
    "                    recon.append(\"\".join([idx2char[idx] for idx in seq]))\n",
    "                    start += ln\n",
    "                print(\"DEBUG: recon[0:4] from targets:\", recon[:4])\n",
    "                # Compare with texts\n",
    "                for i in range(min(4, len(texts))):\n",
    "                    if recon[i] != texts[i]:\n",
    "                        print(f\"WARNING: recon[{i}] != texts[{i}] --> '{recon[i]}' != '{texts[i]}'\")\n",
    "\n",
    "            preds.extend(batch_preds)\n",
    "            gts.extend(texts)\n",
    "\n",
    "            # only debug first batch\n",
    "            break\n",
    "\n",
    "    # compute CER for the partial collected preds/gts\n",
    "    val_cer_debug = cer(preds, gts)\n",
    "    print(\"DEBUG_VAL_CER (first-batch):\", val_cer_debug)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affe002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Final test evaluation using best model\n",
    "# -------------------------\n",
    "ckpt = torch.load(BEST_MODEL_PATH, map_location=DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "all_preds, all_gts = [], []\n",
    "with torch.no_grad():\n",
    "    for images, texts, targets, target_lengths in test_loader:\n",
    "        images = images.to(DEVICE)\n",
    "        logits = model(images)\n",
    "        preds_batch = greedy_decode(logits.cpu(), idx2char)\n",
    "        all_preds.extend(preds_batch)\n",
    "        all_gts.extend(texts)\n",
    "\n",
    "test_cer = cer(all_preds, all_gts)\n",
    "print(\"Final Test CER:\", test_cer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2566a715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']\n"
     ]
    }
   ],
   "source": [
    "class_names = load_class_names(CLASSES_FILE)\n",
    "print(class_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
